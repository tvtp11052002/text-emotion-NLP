```markdown
# ğŸ§  Text Emotion Classification using NLP

A comprehensive NLP project to classify human emotions from text (joy, sadness, anger, fear, neutral) using a combination of traditional machine learning, deep learning, and transformer-based models (BERT). The project includes full preprocessing pipelines, multiple model implementations, and performance comparisons.

## ğŸ“‚ Project Structure

```

text-emotion-nlp/
â”‚
â”œâ”€â”€ finetuned_BERT.ipynb              # Notebook for fine-tuning BERT model
â”œâ”€â”€ lstm.ipynb                  # Notebook for building LSTM/GRU models
â”œâ”€â”€ traditional_ml.ipynb        # Notebook for traditional ML models (SVM, LR, NB, RF)
â”œâ”€â”€ model.png                   # Architecture diagram (optional)
â”‚
â”œâ”€â”€ data/                       # Processed training/testing data
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ test.csv
â”‚   â””â”€â”€ datasets/               # Original datasets
â”‚       â”œâ”€â”€ dailydialog.csv
â”‚       â”œâ”€â”€ emotion-stimulus.csv / .xlsx
â”‚       â”œâ”€â”€ isear.csv / .xlsx
â”‚
â”œâ”€â”€ docx/                       # Final report (PDF + DOCX)
â”‚   â”œâ”€â”€ ProjectNLP.docx
â”‚   â””â”€â”€ ProjectNLP.pdf
â”‚
â”œâ”€â”€ embeddings/                 # Placeholder for pretrained embeddings
â”‚   â””â”€â”€ .gitignore
â”‚
â”œâ”€â”€ models/                     # Saved models
â”‚   â”œâ”€â”€ tfidf\_svm.sav           # Serialized SVM model with TF-IDF
â”‚   â””â”€â”€ emotion\_detector/       # Fine-tuned BERT saved model (TensorFlow format)
â”‚       â”œâ”€â”€ saved\_model.pb
â”‚       â”œâ”€â”€ keras\_metadata.pb
â”‚       â”œâ”€â”€ fingerprint.pb
â”‚       â”œâ”€â”€ assets/
â”‚       â””â”€â”€ variables/

````

## ğŸ“Œ Overview

This project classifies user-generated texts into five emotional categories. It includes data preprocessing, feature extraction using TF-IDF and Word2Vec, and model training with ML algorithms (SVM, NB, etc.), RNN architectures (LSTM, GRU), and transfer learning using fine-tuned BERT. The best model was fine-tuned BERT with over **82% accuracy**.

## âœ… Key Achievements

- Fine-tuned **BERT** model achieved **82.41% accuracy** â€” best among all evaluated models.
- Built GRU model with pretrained FastText embeddings, reaching **71.77% accuracy**.
- Evaluated multiple approaches: NaÃ¯ve Bayes, Logistic Regression, Random Forest, SVM, LSTM, GRU, BERT.
- Full NLP pipeline implemented: text cleaning, tokenization, TF-IDF vectorization, Word2Vec embedding.
- Trained models on **Google Colab (GPU T4)** with proper visualization and evaluation metrics.

## âš™ï¸ Tech Stack

- **Languages**: Python  
- **Libraries**: NLTK, spaCy, Scikit-learn, TensorFlow, Keras, HuggingFace Transformers, Gensim  
- **Tools**: Google Colab, Matplotlib, Seaborn  
- **Models**: NaÃ¯ve Bayes, Logistic Regression, Random Forest, SVM, LSTM, GRU, BERT  
- **Embeddings**: TF-IDF, Word2Vec (wiki-news-300d-1M)

## ğŸ“Š Features

- Dataset preprocessing: cleaning, lemmatization, tokenization
- Feature extraction: TF-IDF and Word2Vec
- ML and DL model training and evaluation
- BERT fine-tuning with HuggingFace Transformers
- Emotion prediction on real-time test inputs
- Saved models for deployment (`/models/`)

## ğŸ§° How to Run

1. Clone the repository  
   ```bash
   git clone https://github.com/yourusername/text-emotion-nlp.git
   cd text-emotion-nlp
````

2. (Optional) Set up virtual environment and install requirements

   ```bash
   pip install -r requirements.txt
   ```

3. Open notebooks:

   * `traditional_ml.ipynb` â†’ train and test ML models
   * `lstm.ipynb` â†’ build and evaluate GRU/LSTM models
   * `finetuned BERT.ipynb` â†’ fine-tune BERT with HuggingFace and TensorFlow

4. Test saved models from `/models/` folder on new messages.

## ğŸ‘¨â€ğŸ’» Authors

* Tráº§n VÄƒn Tuáº¥n Phong
*(Supervised by ÄoÃ n Thá»‹ Há»“ng PhÆ°á»›c â€“ Hue University)*
